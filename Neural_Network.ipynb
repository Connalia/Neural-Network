{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkRed\"> Neural Network by scratch </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κ : targets/categories/class  (πλήθος κατηγοριών)\n",
    "\n",
    "N : all training data   (πλήθος δεδομένων εισόδου)\n",
    "\n",
    "M : hidden units        (ένα κρυφό επίπεδο με  hidden_layer_size = Μ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^{(1)} $πίνακας βαρών με μέγεθος: $Μ x (D + 1)$\n",
    "\n",
    "$W^{(2)} $πίνακας βαρών με μέγεθος: $K x M + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Image/myMLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image From:https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6 (for 2 layers MLP)\n",
    "\n",
    "Mapping with my MLP:\n",
    "\n",
    "- σ = h :Activation Function\n",
    "\n",
    "- Loss = Cost Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTex for maths in notebook\n",
    "https://www.math.ubc.ca/~pwalls/math-python/jupyter/latex/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need for cifar dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {non linear function}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for hidden layer:\n",
    "\n",
    " $$h(.) = **Activation Function**$$\n",
    "\n",
    "(1)softplus function:$$h(α) = log(1 + e^α)$$ \n",
    "\n",
    "(2)$$ h(α) = \\frac{e^α− e^{−α}}{e^α + e^{−α}} = \\frac {\\frac{e^α− e^{−α}}{2}}{\\frac{e^α + e^{−α}}{2}}= \\frac{sin(a)}{cos(a)}= tanh(a) $$\n",
    "\n",
    "(3)$$ h(α) = cos(α)(=\\frac{e^α + e^{−α}}{2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activationLog(a):\n",
    "    h = np.log( 1 + np.exp(a))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activationDiv(a):\n",
    "    #h =  (np.exp(a) - np.exp(-a)) / (np.exp (a) + np.exp(-a)) //Not safe-overflow\n",
    "    #TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
    "    h = np.tanh(a)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activationCos(a):\n",
    "    h =  np.cos(a)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(activID,a):\n",
    "    \"\"\"\n",
    "    Execute the activation on input data\n",
    "    :param activID: The id of the activation we choose\n",
    "    :param a: Out input that will pass through the activation\n",
    "\n",
    "    :return: The result of the activation\n",
    "    \"\"\"\n",
    "\n",
    "    if activID=='1':\n",
    "        h = activationLog(a)\n",
    "    elif activID=='2':\n",
    "        h = activationDiv(a)\n",
    "    elif activID=='3':\n",
    "        h = activationCos(a)\n",
    "    else : print(\"you don't give number 1,2 or 3\")\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function for output layer:\n",
    "\n",
    "$$softmax(a_i) = \\frac{ e^{a_i} }{ \\sum_{i=1}^{K} e^{ a_i } }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use by default ax=1, when the array is 2D\n",
    "#use ax=0 when the array is 1D\n",
    "def activationSoftmax( z, ax=1):\n",
    "    m = np.max( z, axis=ax, keepdims=True )#max per row\n",
    "    p = np.exp( z - m )\n",
    "    return ( p / np.sum(p,axis=ax,keepdims=True) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {linear function}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense Layer\n",
    "\n",
    "$$f(x ; w)= W^{T} X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer:\n",
    "{activationFunct: no linear function} with {f : linear function} => {z : no linear function}\n",
    "\n",
    "$$ z = h(f) = h ( W^{T} X) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output layer: use activation function **softmax function** (input is the output from hidden layer)\n",
    "\n",
    "$$softmax(a_i) = \\frac{ e^{a_i} }{ \\sum_{i=1}^{K} e^{ a_i } }$$\n",
    "\n",
    "$$=> y_{nk} = \\frac{ e^{(w_{k}^{(2)})^{T} z_{n}} }{ \\sum_{j=1}^{K} e^{(w_{j}^{(2)})^{T} z_{n} } }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(W1, W2, X, activID, lamda, t):\n",
    "\n",
    "    # Hidden layer\n",
    "    f1 = X.dot(W1.T)\n",
    "    Z = activation(activID, f1)\n",
    "\n",
    "    # add bias #Append a column with aces at the start\n",
    "    ones = np.ones((Z.shape[0], 1))\n",
    "    Z = np.append(ones, Z, axis=1)\n",
    "\n",
    "    # Output layer\n",
    "    f2 = Z.dot(W2.T)\n",
    "    max_error = np.max(f2, axis=1)\n",
    "\n",
    "    Y = activationSoftmax(f2)\n",
    "\n",
    "\n",
    "    # cost function #mathematical analysis below\n",
    "    Cost = np.sum(t * f2) - np.sum(max_error) - \\\n",
    "           np.sum(np.log(np.sum(np.exp(f2 - np.array([max_error, ] * f2.shape[1]).T), 1))) - \\\n",
    "           ((1 / 2) * lamda) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "\n",
    "\n",
    "    return Z, Y, Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(logLikelihood plus reguralization term)\n",
    "\n",
    "we want to maximize for the problem of classifying N number of data in K categories/classes is:\n",
    "\n",
    "$$\n",
    "E(W) = \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log y_{nk}   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w_k}||^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "=> E(W) = \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log (\\frac{e^{\\mathbf{w}_k^T \\mathbf{z}_n}}{\\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{z}_n}})   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w_k}||^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "=>E(W) = \\sum_{n=1}^N \\left[  \\sum_{k=1}^K t_{nk} \\log (e^{\\mathbf{w}_k^T \\mathbf{z}_n})  - \\log \\left( \\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{z}_n} \\right) \\right]   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w}_k||^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "=> E(W) = \\sum_{n=1}^N \\left[ \\left( \\sum_{k=1}^K t_{nk} \\mathbf{w}_k^T \\mathbf{z}_n \\right) - \\log \\left( \\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{z}_n} \\right) \\right]   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w}_k||^2, \n",
    "$$\n",
    "\n",
    "where $y_{nk}$ is the softmax function defined as:\n",
    "\n",
    "$$y_{nk} = \\frac{e^{\\mathbf{w}_k^T \\mathbf{z}_n}}{\\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{z}_n}}$$\n",
    "$W$ is a $K \\times (D+1)$ matrix where each line represents the vector $\\mathbf{w}_k$.\n",
    "\n",
    "\n",
    "The cost function can be simplified in the following form:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "E(W) = \\sum_{n=1}^N \\left[ \\left( \\sum_{k=1}^K t_{nk} \\mathbf{w}_k^T \\mathbf{z}_n \\right) - \\log \\left( \\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{z}_n} \\right) \\right]   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w}_k||^2, \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "In the above formula we have used the fact that $\\sum_{k=1}^K t_{nk} = 1$. \n",
    "\n",
    "The partial derrivatives of this function are given by the following $K \\times (D+1)$ matrix:\n",
    "\n",
    "$$\n",
    "(T - Y)^Τ Z - \\lambda W,\n",
    "$$\n",
    "\n",
    "**!όλοι αυτή η συνάρτηση ΕΙΝΑΙ Η ΜΕΡΙΚΗ ΠΑΡΑΓΩΓΟΣ της συναρτησεις κόστους!**\n",
    "\n",
    "where:\n",
    "\n",
    "$T$ is an $N \\times K$ matrix with the truth values of the training data, such that $[T]_{nk} = t_{nk}$, \n",
    "\n",
    "$Y$ is the corresponding $N \\times K$ matrix that holds the softmax probabilities such that $[Y]_{nk} = y_{nk}$, \n",
    "\n",
    "$Z$ is the $N \\times (D + 1)$ matrix of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the logsumexp trick, where m is the maximum element\n",
    "\\begin{align} \n",
    "\\log \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{z}_n} &= \\log \\Bigr( \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{z}_n +m -m}\\Bigl) \\\\ \n",
    "&= \\log \\Bigr( \\sum_{i=1}^{n} e^m e^{\\mathbf{w}_j^T \\mathbf{z}_n-m} ) \\Bigl) \\\\ \n",
    "&= \\log \\Bigr( e^m \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{z}_n-m} ) \\Bigl) \\\\ \n",
    "&= \\log \\ e^m + \\log \\Bigr( \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{z}_n-m} ) \\Bigl) \\\\ \n",
    "&= m + \\log \\Bigr( \\sum_{i=1} e^{\\mathbf{w}_j^T \\mathbf{z}_n-m}  \\Bigl) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKPROGATION(Stochastic gradient ascent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateWeights(W, lr, grad):\n",
    "    \n",
    "    W = W + lr * grad\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives (Μερικοί Παράγωγοι)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights layer 2 (output)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Derivative for Cost Function (ΜΕΡΙΚΗ ΠΑΡΑΓΩΓΟΣ της συναρτησεις κόστους)\n",
    "\n",
    "EN: Some derivatives for the parameters W (2) have a form similar to that of simple linear logistic regression of many categories and are given by the relation:\n",
    "\n",
    "GR: Οι μερικές παράγωγοι για τις παραμέτρους W (2) έχουν όμοια μορφή με αυτή της απλής γραμμικής λογιστικής παλινδρόμησης πολλών κατηγοριών και δίνονται από την σχέση:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Cost}{\\partial{W_2}} =(T - Y)^Τ Z - \\lambda W_2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientW2(W2 , Y, Z , T ,lamda):\n",
    "    # calculate gradient\n",
    "    gradientW2 = (T - Y).T.dot(Z) - lamda * W2\n",
    "    return gradientW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights layer 1 (hidden)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Derivative for Activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)$$h(α) = log(1 + e^α)$$\n",
    "\n",
    "$$\\frac{\\partial h(α)}{\\partial{a}} = \\frac{\\partial(log(1 + e^α))}{\\partial{α}}$$\n",
    "Apply chain rule \n",
    "\n",
    "(rememder $(log(x))'= \\frac {1}{1 + x}$):\n",
    "$$ = \\frac {1}{1 + e^a} (1 + e^a)'$$\n",
    "\n",
    "$$= \\frac {1}{1 + e^a} e^a$$\n",
    "\n",
    "$$= \\frac {e^a}{1 + e^a} $$\n",
    "\n",
    "$$= \\frac {e^a}{e^a(\\frac {1}{e^a} + 1)} $$\n",
    "\n",
    "$$= \\frac {1}{\\frac {1}{e^a} + 1} $$\n",
    "$$= \\frac {1}{e^{-a} + 1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeLog(a):\n",
    "    d = 1 / ( np.exp(-a) + 1 )\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)$$ h(α) = \\frac{e^α− e^{−α}}{e^α + e^{−α}} $$\n",
    "\n",
    "$$\\frac{\\partial h(α)}{\\partial{α}} = (\\frac{e^α− e^{−α}}{e^α + e^{−α}})'$$\n",
    "$$= \\frac {(e^α − e^{−α})'(e^α + e^{−α}) - (e^α − e^{−α})(e^α + e^{−α})'}{(e^α + e^{−α})^2}$$\n",
    "$$= \\frac {(e^α + e^{−α})(e^α + e^{−α}) - (e^α − e^{−α})(e^α - e^{−α})}{(e^α + e^{−α})^2}$$\n",
    "$$= \\frac {(e^α + e^{−α})^2 - (e^α − e^{−α})^2}{(e^α + e^{−α})^2}$$\n",
    "$$= 1-\\frac {(e^α − e^{−α})^2}{(e^α + e^{−α})^2}$$\n",
    "$$= 1-(\\frac {e^α − e^{−α}}{e^α + e^{−α}})^2$$\n",
    "$$= 1- tanh(a)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeDiv(a):\n",
    "    #d = 4 / ((np.exp(a) + np.exp(-a))^2)//Not safe-overflow\n",
    "    d = 1 - np.power(np.tanh(a), 2)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)$$ h(α) = cos(α)$$\n",
    "\n",
    "$$(cos(α))' = - sin(a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeCos(a):\n",
    "    d = - np.sin(a)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_activation(activID, a):\n",
    "    \"\"\"\n",
    "    Execute the activation on input data\n",
    "    inputs :\n",
    "    :param activID: The id of the activation we choose\n",
    "    :param a: Out input that will pass through the deravate activation\n",
    "\n",
    "    output:\n",
    "    :return: The result of the derivate of activation\n",
    "    \"\"\"\n",
    "\n",
    "    if activID == '1':\n",
    "        dh = derivativeLog(a)\n",
    "    elif activID == '2':\n",
    "        dh = derivativeDiv(a)\n",
    "    elif activID == '3':\n",
    "        dh = derivativeCos(a)\n",
    "    else:\n",
    "        print(\"you don't give number 1,2 or 3\")\n",
    "\n",
    "    return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: \n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <th>//Hidden Layer</th>\n",
    "    <th> //Output Layer</th>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$f1=W1^{T}X$</td>\n",
    "    <td>$f2=W2^{T}Z$</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$Z = h(f1)$</td>\n",
    "    <td>$Y = softmax(f2)$</td>\n",
    "    \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Cost}{\\partial{Z}} = \\frac{\\partial Cost}{\\partial{Y}} * \\frac{\\partial Y}{\\partial{f2}} * \\frac{\\partial f2}{\\partial{Z}}$ (chain rule)&nbsp;&nbsp;&nbsp;  $[\\frac{\\partial f2}{\\partial{Z}}= W2]$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial{f1}} = eg.-sin(f1)$&nbsp;(2)&nbsp;//derevate of activation\n",
    "\n",
    "$\\frac{\\partial f1}{\\partial{W1}} = X $&nbsp;(3)&nbsp;//derevate of linear\n",
    "\n",
    "=> $ \\frac{\\partial Cost}{\\partial{W1}} = \\frac{\\partial Cost}{\\partial{Z}} * \\frac{\\partial z}{\\partial{f1}} * \\frac{\\partial f1}{\\partial{W1}}$&nbsp;(4) &nbsp;(chain rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have\n",
    "\n",
    "$\\frac{\\partial Cost}{\\partial{Z}} = (T-Y) * W2 $ (1)\n",
    "\n",
    "(1)*(2) $\\frac{\\partial Cost}{\\partial{Z}} * \\frac{\\partial z}{\\partial{f1}} = ((T-Y) * W2)* (eg.-sin (f1))$&nbsp;(3)\n",
    "\n",
    "(1)*(2)*(3) $\\frac{\\partial Cost}{\\partial{Z}} * \\frac{\\partial z}{\\partial{f1}} * \\frac{\\partial f1}{\\partial{W1}} = ((T-Y)* W2)^{T}* (eg.-sin (f1))* X - λW1$ &nbsp;(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientW1(W1, W2, X, activID, lamda, T, Y):\n",
    "    # Remove the bias\n",
    "    w2_noBias = np.copy(W2[:, 1:])\n",
    "\n",
    "    f1 = X.dot(W1.T)\n",
    "    dz = derivate_activation(activID, f1)\n",
    "\n",
    "    dc = (T - Y).dot(w2_noBias)\n",
    "\n",
    "    gradiendW1 = (dc * dz).T.dot(X) - lamda * W1\n",
    "\n",
    "    return gradiendW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EN: The function `gradCheck` compare the exact gradients with \n",
    "numerical estimates obtained by finite differences\n",
    "\n",
    "GR: Η συνάρτηση `gradCheck` που συγκρίνει τις αναλυτικές παραγώγους με αριθμητικές διαφορές\n",
    "\n",
    "(μέθοδος για να σιγουρευτούμε ότι οι παραγωγει που υπολογίσαμε είναι σωστές)\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial{w_ij}}= \\frac{E(w_{ij}+ε)-E(w_{ij}-ε)}{2ε}$$\n",
    "\n",
    "$E= Cost$ \n",
    "\n",
    "and \n",
    "\n",
    "$ε=epsilon= 10^{-6}=1e-6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCheck(W1, W2, X, t, lamda, activID):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    :param W1: Initial weights of Hidden Layer\n",
    "    :param W2: Initial weights of Output Layer\n",
    "    :param X: Our data\n",
    "    :param t: The corresponding labels\n",
    "    :param lamda:(regularization term)\n",
    "    :param activID: The id of the activation we choose\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    onesX = np.ones((X.shape[0], 1))\n",
    "    X = np.append(onesX, X, axis=1)\n",
    "\n",
    "    _list = np.random.randint(X.shape[0], size=5)\n",
    "    x_sample = np.array(X[_list, :])\n",
    "    t_sample = np.array(t[_list, :])\n",
    "\n",
    "    # Feed Forward\n",
    "    Z, Y, Cost = feedForward(W1, W2, x_sample, activID, lamda, t_sample)\n",
    "\n",
    "    # Backprogation # Stocastic gradient\n",
    "    gradW2 = gradientW2(W2, Y, Z, t_sample, lamda)\n",
    "    gradW1 = gradientW1(W1, W2, x_sample, activID, lamda, t_sample, Y)\n",
    "\n",
    "    numericalGrad = np.zeros(gradW1.shape)\n",
    "    # Compute all numerical gradient estimates and store them in\n",
    "    # the matrix numericalGrad\n",
    "    for k in range(numericalGrad.shape[0]):\n",
    "        for d in range(numericalGrad.shape[1]):\n",
    "            # add epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W1)\n",
    "            w_tmp[k, d] += epsilon\n",
    "            _, _, e_plus = feedForward(w_tmp, W2, x_sample, activID, lamda, t_sample)  # Feed Forward\n",
    "\n",
    "            # subtract epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W1)\n",
    "            w_tmp[k, d] -= epsilon\n",
    "            _, _, e_minus = feedForward(w_tmp, W2, x_sample, activID, lamda, t_sample)  # Feed Forward\n",
    "\n",
    "            # approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "\n",
    "    # Absolute norm\n",
    "    print(\"The difference estimate for gradient of w1 is : \", np.max(np.abs(gradW1 - numericalGrad)))\n",
    "\n",
    "    numericalGrad = np.zeros(gradW2.shape)\n",
    "    # Compute all numerical gradient estimates and store them in\n",
    "    # the matrix numericalGrad\n",
    "    for k in range(numericalGrad.shape[0]):\n",
    "        for d in range(numericalGrad.shape[1]):\n",
    "            # add epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W2)\n",
    "            w_tmp[k, d] += epsilon\n",
    "            _, _, e_plus = feedForward(W1, w_tmp, x_sample, activID, lamda, t_sample)  # Feed Forward\n",
    "\n",
    "            # subtract epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W2)\n",
    "            w_tmp[k, d] -= epsilon\n",
    "            _, _, e_minus = feedForward(W1, w_tmp, x_sample, activID, lamda, t_sample)  # Feed Forward\n",
    "\n",
    "            # approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "    # Absolute norm\n",
    "    print(\"The difference estimate for gradient of w2 is : \", np.max(np.abs(gradW2 - numericalGrad)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Μore detail**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During gradient ascent/descent we compute the gradients $\\frac{\\partial E}{\\partial w}$, where $w$ denotes the parameters of the model.\n",
    "\n",
    "In order to make sure that these gradients are correct we will compare the exact gradients(that we have coded) with numerical estimates obtained by finite differences, you can use your code for computing $E$ to verify the code for computing $\\frac{\\partial E}{\\partial w}$.\n",
    "    Let's look back at the definition of a derivative (or gradient):\n",
    "    \n",
    "$$ \\frac{\\partial E}{\\partial w} = \\lim_{\\varepsilon \\to 0} \\frac{E(w + \\varepsilon) - E(w - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$  \n",
    "\n",
    "We know the following: \n",
    "- $\\frac{\\partial E}{\\partial w}$ is what you want to make sure you're computing correctly. ,\n",
    "- You can compute $E(w + \\varepsilon)$ and $E(w - \\varepsilon)$ (in the case that $w$ is a real number), since you're confident your implementation for $E$ is correct.\n",
    "\n",
    "Let's use equation (1) and a small value ( around $10^-4$ or $10^-6$, much smaller values could lead to numerical issues )for $\\varepsilon$ to make sure that your code for computing  $\\frac{\\partial E}{\\partial w}$ is correct!\n",
    "\n",
    "![title](Image/grad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W1, W2, x_train, y_train, lr, epochs, batch_size, lamda, activID):\n",
    "    \"\"\"\n",
    "    Training process\n",
    "\n",
    "    inputs :\n",
    "    :param w1: Hidden Layer weights\n",
    "    :param w2: Output Layer weights\n",
    "\n",
    "    :param x_train: Table for training data N x D . N: number of data, D: dimension\n",
    "    :param y_train: Table for training labels N x d\n",
    "\n",
    "    :param lr: learning rate\n",
    "\n",
    "    :param epochs: number of epochs\n",
    "    :param  batch_size: batch size\n",
    "\n",
    "    :param lamda: value of lamda (regularization term)\n",
    "\n",
    "    :param activID: The id of the activation we choose\n",
    "\n",
    "    output :\n",
    "    :return: Trained weight of the network to use them for predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # add bias\n",
    "    # add column of ones to the dataset for multiply in linear function: 1* w0\n",
    "    # f(x)= 1*w0 + x*w1...\n",
    "    onesArr = np.ones((x_train.shape[0], 1))\n",
    "    X_train = np.append(onesArr, x_train, axis=1)\n",
    "\n",
    "    # decay learning rate\n",
    "    # towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "    lr = lr / epochs\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        print(\"Epoch:\" + str(i + 1) + '/' + str(epochs))\n",
    "\n",
    "        # zip the feature and the labels, so shuffle with same way\n",
    "        connectArray = list(zip(X_train, y_train))\n",
    "\n",
    "        # shuffle: the same examples avoid to be in the same batches\n",
    "        np.random.shuffle(connectArray)\n",
    "\n",
    "        X_train, y_train = zip(*connectArray)\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        # Train of each batch\n",
    "        for j in range(0, X_train.shape[0], batch_size):\n",
    "            batch_x = X_train[j: j + batch_size, :]  # is X\n",
    "            batch_y = y_train[j: j + batch_size, :]  # is T\n",
    "\n",
    "            # Neural Network:\n",
    "\n",
    "            # Feed Forward\n",
    "            Z, Y, Cost = feedForward(W1, W2, batch_x, activID, lamda, batch_y)\n",
    "\n",
    "            # Backprogation # Stocastic gradient\n",
    "            W2_grad = gradientW2(W2, Y, Z, batch_y, lamda)\n",
    "            W1_grad = gradientW1(W1, W2, batch_x, activID, lamda, batch_y, Y)\n",
    "\n",
    "            W1 = updateWeights(W1, lr, W1_grad)\n",
    "            W2 = updateWeights(W2, lr, W2_grad)\n",
    "\n",
    "    return W1, W2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a python3 routine which will open such a file and return a dictionary of cifar dataset:\n",
    "    https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def unpickle(file):\n",
    "    \"\"\"\n",
    "    Here is a python3 routine which will open such a file and return a dictionary:\n",
    "    https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    \n",
    "    input:\n",
    "    :param file: Path of dataset\n",
    "    output:\n",
    "    :return: Dictionary to dataset\n",
    "    \"\"\"\n",
    "    with open(file, 'rb') as fo:\n",
    "        dictionary = pickle.load(fo, encoding='bytes')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical element:\n",
    "\n",
    "For each element in dataset belong to only one target k.\n",
    "\n",
    "So we have a binary vector of length K.\n",
    "\n",
    "$t_{nk} \\in \\{ 0,1 \\}, \\sum_{k=1}^{K} t_{nk} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(datasetID):\n",
    "    \"\"\"\n",
    "    inputs :\n",
    "    :param datasetID: Id of the dataset thato user want to load\n",
    "                        1 -> mnist or 2 -> cifar\n",
    "    \"\"\"\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "\n",
    "    if datasetID == '1':\n",
    "        # load the train files\n",
    "        df = None\n",
    "        y_train = []\n",
    "        for i in range(10):\n",
    "\n",
    "            tmp = pd.read_csv('Data/mnist/train%d.txt' % i, header=None, sep=\" \")\n",
    "\n",
    "            # one hot vector for labels (y_train)\n",
    "            hot_vector = [1 if j == i else 0 for j in range(0, 10)]\n",
    "            for j in range(tmp.shape[0]):\n",
    "                y_train.append(hot_vector)\n",
    "\n",
    "            # concatenate dataframes by rows\n",
    "            # concat all train txt from mnist\n",
    "            if i == 0:\n",
    "                df = tmp\n",
    "            else:\n",
    "                df = pd.concat([df, tmp])\n",
    "\n",
    "        x_train = df.as_matrix()\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        # load test files\n",
    "        df = None\n",
    "        y_test = []\n",
    "        for i in range(10):\n",
    "\n",
    "            tmp = pd.read_csv('Data/mnist/test%d.txt' % i, header=None, sep=\" \")\n",
    "\n",
    "            # one hot vector for labels (y_test)\n",
    "            hot_vector = [1 if j == i else 0 for j in range(0, 10)]\n",
    "            for j in range(tmp.shape[0]):\n",
    "                y_test.append(hot_vector)\n",
    "\n",
    "            # concatenate dataframes by rows\n",
    "            # concat all test txt from mnist\n",
    "            if i == 0:\n",
    "                df = tmp\n",
    "            else:\n",
    "                df = pd.concat([df, tmp])\n",
    "\n",
    "        x_test = df.as_matrix()\n",
    "        y_test = np.array(y_test)\n",
    "\n",
    "        # tranform the data of image to scale[0,1]\n",
    "        x_train = x_train.astype(float) / 255\n",
    "        x_test = x_test.astype(float) / 255\n",
    "\n",
    "        return x_train, x_test, y_train, y_test\n",
    "\n",
    "    elif datasetID == '2':\n",
    "        # load the train files\n",
    "        for i in range(5):\n",
    "            path = 'Data/cifar-10-batches-py/data_batch_{0:d}'.format(i+1)\n",
    "            batch_i = unpickle(path)\n",
    "\n",
    "            for j in range(len(batch_i['data'.encode('ascii', 'ignore')])):\n",
    "                x_train.append(batch_i['data'.encode('ascii', 'ignore')][j])\n",
    "                labels = batch_i['labels'.encode('ascii', 'ignore')][j]\n",
    "\n",
    "                # one hot vector for labels (y_train)\n",
    "                hot_vector = [1 if k == labels else 0 for k in range(0, 10)]\n",
    "                y_train.append(hot_vector)\n",
    "\n",
    "        # load the test file\n",
    "        path = 'Data/cifar-10-batches-py/test_batch'\n",
    "        test_batch = unpickle(path)\n",
    "\n",
    "        for j in range(len(test_batch['data'.encode('ascii', 'ignore')])):\n",
    "            x_test.append(test_batch['data'.encode('ascii', 'ignore')][j])\n",
    "            labels = test_batch['labels'.encode('ascii', 'ignore')][j]\n",
    "\n",
    "            # one hot vector for labels (y_test)\n",
    "            hot_vector = [1 if k == labels else 0 for k in range(0, 10)]\n",
    "            y_test.append(hot_vector)\n",
    "\n",
    "    return np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select a dataset id: \n",
      "id : dataset \n",
      "-------------\n",
      "1  : MNIST \n",
      "2  : CIFAR-10 \n",
      "Type here: 1\n"
     ]
    }
   ],
   "source": [
    "datasetID = input(\"\\nSelect a dataset id: \\n\"\n",
    "                        \"id : dataset \\n\"\n",
    "                        \"-------------\\n\"\n",
    "                        \"1  : MNIST \\n\"\n",
    "                        \"2  : CIFAR-10 \\n\"\n",
    "                        \"Type here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Connalia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Connalia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "if datasetID == '2':\n",
    "    x_train, x_test, y_train, y_test = loadDataset(datasetID)\n",
    "else:\n",
    "    if datasetID != '1':\n",
    "        print(\"By default selected the MNIST dataset\")\n",
    "    x_train, x_test, y_train, y_test = loadDataset(datasetID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select a activation function id for hidden layer: \n",
      "id:  activation \n",
      "----------------\n",
      "1 : log(1+exp(a))\n",
      "2 :(exp(a)-exp(-a))/((a)+exp(-a))\n",
      "3 : cos(a) \n",
      "Type here: 3\n"
     ]
    }
   ],
   "source": [
    "activID = input(\"\\nSelect a activation function id for hidden layer: \\n\"\n",
    "                        \"id:  activation \\n\"\n",
    "                        \"----------------\\n\"\n",
    "                        \"1 : log(1+exp(a))\\n\"\n",
    "                        \"2 :(exp(a)-exp(-a))/((a)+exp(-a))\\n\"\n",
    "                        \"3 : cos(a) \\n\"\n",
    "                        \"Type here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default activation id\n",
    "if activID != '1' and activID != '2' and activID != '3':\n",
    "    print(\"By default selected the activation function cos(a)\")\n",
    "    activID = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select size of minibatches: \n",
      "id:  batch size \n",
      "----------------\n",
      "1 : 100\n",
      "2 : 200\n",
      "Type here: 1\n"
     ]
    }
   ],
   "source": [
    "batch_sizeID = input(\"\\nSelect size of minibatches: \\n\"\n",
    "                         \"id:  batch size \\n\"\n",
    "                         \"----------------\\n\"\n",
    "                         \"1 : 100\\n\"\n",
    "                         \"2 : 200\\n\"\n",
    "                         \"Type here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default size of minibatches\n",
    "if batch_sizeID == '2':\n",
    "    batch_size = 200\n",
    "else:\n",
    "    if batch_sizeID != '1':\n",
    "        print(\"By default selected batch size = 100\")\n",
    "    batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select number of neurons in the hidden layer: \n",
      "id:  neurons \n",
      "----------------\n",
      "1 : 100\n",
      "2 : 200\n",
      "3 : 300\n",
      "Type here: 1\n"
     ]
    }
   ],
   "source": [
    "M = input(\"\\nSelect number of neurons in the hidden layer: \\n\"\n",
    "              \"id:  neurons \\n\"\n",
    "              \"----------------\\n\"\n",
    "              \"1 : 100\\n\"\n",
    "              \"2 : 200\\n\"\n",
    "              \"3 : 300\\n\"\n",
    "              \"Type here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default number of neurons\n",
    "if M == '2':\n",
    "    M = 200\n",
    "elif M == '3':\n",
    "    M = 300\n",
    "else:\n",
    "    if M != '1':\n",
    "        print(\"By default selected 100 neurons\")\n",
    "    M = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization parameter in CostFunction\n",
    "lamda = 0.1\n",
    "\n",
    "# learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# number of epochs\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mnists consists of $28x28$ grayscale images, so the image have $784$ pixels.\n",
    "\n",
    "In total there are 10 training files (train0.txt, train1.txt, ..., train9.txt) where each rows of train$k$.txt corresponds to an example that belongs to the category $k$.\n",
    "\n",
    "* So we have 10 targets/categories in mnist: 0-9\n",
    "\n",
    "\n",
    "Cifar consists of $32x32$ images, so the image have $1024$ pixels.\n",
    "* So we have 10 targets/categories in cifar: 1.airplane,2.automobile,3.bird,4.cat,5.deer,6.dog,7.frog,8.horse,9.ship,10.truck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of categories\n",
    "K = 10\n",
    "\n",
    "# D: num of input pixels : eg mnist 28*28 = 784\n",
    "# N: num of training examples\n",
    "N, D = x_train.shape\n",
    "\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# follow normal distribution\n",
    "center = 0\n",
    "s = 1 / np.sqrt(D + 1)\n",
    "W1 = np.random.normal(center, s, (M, D + 1))\n",
    "\n",
    "# random init\n",
    "W2 = np.random.rand(K, M + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference estimate for gradient of w1 is :  5.969499261571087e-08\n",
      "The difference estimate for gradient of w2 is :  4.08211193736463e-08\n"
     ]
    }
   ],
   "source": [
    "gradCheck(W1, W2, x_train, y_train, lamda, activID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/5\n",
      "Epoch:2/5\n",
      "Epoch:3/5\n",
      "Epoch:4/5\n",
      "Epoch:5/5\n"
     ]
    }
   ],
   "source": [
    "W1_train, W2_train = train(W1, W2, x_train, y_train, lr, epochs, batch_size, lamda, activID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y,_ = function()` means that the function return two parameters,but we want only `y` \n",
    "\n",
    "(here predict=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the bias on test data\n",
    "onesArrTest = np.ones((x_test.shape[0], 1))\n",
    "x_test = np.append(onesArrTest, x_test, axis=1)\n",
    "\n",
    "_, predict, Cost = feedForward(W1_train, W2_train, x_test, activID, lamda, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurancy: 0.9728\n"
     ]
    }
   ],
   "source": [
    "predict = np.argmax(predict, 1)\n",
    "real = np.argmax(y_test, 1)\n",
    "\n",
    "accurancy =np.mean( predict == np.argmax(y_test,1) )\n",
    "\n",
    "print(\"Accurancy: \" +str(accurancy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
